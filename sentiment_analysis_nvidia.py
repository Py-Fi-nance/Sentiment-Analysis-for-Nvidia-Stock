# -*- coding: utf-8 -*-
"""2Sentiment Analysis Nvidia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ezE0sYOvowT72MdXqycGKLZcnCsyS_w9
"""

!pip install GoogleNews
!pip install fake-useragent
!pip install newspaper3k

# Import necessary libraries
from GoogleNews import GoogleNews
import pandas as pd

# Define the keyword to search
keyword = 'Nvidia'

# Collect all results
all_results = []
for period in ['7']: # You can adjust the time frames
    # Create GoogleNews object with specific language, region, period, and encoding
    googlenews = GoogleNews(lang='en', region='US', period=period, encode='utf-8')

    # Search with the specific keyword
    googlenews.search(keyword)

    # Iterate through several pages (you can adjust the range)
    for page in range(1, 20):
        googlenews.get_page(page)
        all_results.extend(googlenews.results())

    # Clear search results for the next iteration
    googlenews.clear()

# Convert the results to a Pandas DataFrame
news_data_df = pd.DataFrame.from_dict(all_results)

# Display information of the DataFrame
news_data_df.info()

# Display header of dataframe.
news_data_df.head()

news_data_df.tail()

import requests
from fake_useragent import UserAgent
import newspaper
from newspaper import fulltext
import re

ua = UserAgent()
news_data_df_with_text = []
for index, headers in news_data_df.iterrows():
    news_title = str(headers['title'])
    news_media = str(headers['media'])
    news_update = str(headers['date'])
    news_timestamp = str(headers['datetime'])
    news_description = str(headers['desc'])
    news_link = str(headers['link'])
    print(news_link)
    news_img = str(headers['img'])
    try:
        # html = requests.get(news_link).text
        html = requests.get(news_link, headers={'User-Agent':ua.chrome}, timeout=5).text
        text = fulltext(html)
        print('Text Content Scraped')
    except:
        print('Text Content Scraped Error, Skipped')
        pass
    news_data_df_with_text.append([news_title, news_media, news_update, news_timestamp,
                                         news_description, news_link, news_img, text])

news_data_with_text_df = pd.DataFrame(news_data_df_with_text, columns=['Title', 'Media', 'Update', 'Timestamp',
                                                                    'Description', 'Link', 'Image', 'Text'])

# Save the result dataframe into a CSV file.
news_data_with_text_df.to_csv("./news_data_with_text.csv")

# Reload the saved news data content from a CSV file.
news_data_with_text_df1 = pd.read_csv("/content/news_data_with_text.csv",  index_col=0)

!pip install openbb

import pandas as pd
from openbb_terminal.sdk import openbb

nvda_df = openbb.stocks.load(symbol = 'nvda')

nvda_df.head()

nvda_df.tail()

!pip install textblob
from textblob import TextBlob

# Create a function to get the subjectivity
def getSubjectivity(text):
    return TextBlob(str(text)).sentiment.subjectivity

# Create a function to get the polarity
def getPolarity(text):
    return TextBlob(str(text)).sentiment.polarity

# Create two new columns 'Subjectivity' & 'Polarity'
news_data_with_text_df1['Subjectivity'] = news_data_with_text_df1['Text'].apply(getSubjectivity)
news_data_with_text_df1['Polarity'] = news_data_with_text_df1['Text'].apply(getPolarity)

def check_date(date):
    try:
        pd.to_datetime(date)
        return True
    except ValueError:
        return False

# Apply the check_date function to the "Datetime" column to filter out invalid dates
news_data_with_text_df1 = news_data_with_text_df1[news_data_with_text_df1["Timestamp"].apply(check_date)]

# Convert 'Datetime' to datetime
news_data_with_text_df1['Timestamp'] = pd.to_datetime(news_data_with_text_df1['Timestamp'])

# Extract date from 'Datetime'
news_data_with_text_df1['Date'] = news_data_with_text_df1['Timestamp'].dt.date

# Then you can group by 'Date'
News_df_daily = news_data_with_text_df1.groupby('Date').mean().reset_index()

News_df_daily['Date'] = pd.to_datetime(News_df_daily['Date'])
News_df_daily.set_index('Date', inplace=True)

print("nvda_df date range: ", nvda_df.index.min(), "to", nvda_df.index.max())
print("News_df_daily Date range: ", News_df_daily.index.min(), "to", News_df_daily.index.max())

merged_df = nvda_df.merge(News_df_daily, left_index=True, right_index=True, how='inner')
print(merged_df.shape)

plt.figure(figsize=(10, 6))
plt.scatter(News_df_daily.index, News_df_daily['Polarity'], label='Polarity')
plt.plot(News_df_daily.index, News_df_daily['Polarity'], color='blue', alpha=0.5)
plt.title('Sentiment Polarity over Time')
plt.xlabel('Date')
plt.ylabel('Polarity')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Slice the DataFrame to only include data from 2023-08-02 onwards
nvda_subset = nvda_df['2023-08-01':]

# Plotting the closing price for the subset
plt.figure(figsize=[15,7])
plt.plot(nvda_subset['Close'])
plt.title('NVIDIA Closing Price Over Time (from 2023-08-02)')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.grid(True)
plt.show()